# ğŸ¬ Talk2Video

**Talk2Video** is an open-source AI agent that transforms spoken or written instructions into narrated videos.  
Say it â€” see it. Just like that.

Whether it's a story, presentation, tutorial, or visual script, Talk2Video listens, understands, and **directs your video with speech**.

---

## âœ¨ Features

- ğŸ™ï¸ Convert **spoken or typed commands** into full video scenes
- ğŸ§  Use **LLM (OpenAI, Mistral, etc.)** to understand and script scenes
- ğŸ“¸ Generate video scenes with **text-to-image/video models** (DALLÂ·E, SD, Pika, RunwayML)
- ğŸ—£ï¸ Add natural voiceovers with **TTS APIs** (ElevenLabs, Bark, Coqui, Google TTS)
- ğŸï¸ Compose and render final video with **FFmpeg** or **moviepy**
- ğŸ§¾ Script memory: multi-step stories or presentations
- ğŸ›ï¸ Optional GUI and CLI versions
- ğŸŒ **Multi-language support** (English, Spanish, French, German, Japanese, Turkish)
- ğŸ¨ Multiple scene styles (cinematic, documentary, commercial, educational, dramatic)
- ğŸ­ Customizable voice parameters (speed, pitch)

---

## ğŸ§  Architecture & Tech Stack

| Layer | Tools |
|-------|-------|
| ğŸ—£ï¸ Input | Microphone (speech) or text prompt |
| ğŸ’¬ NLP & Scene Logic | OpenAI GPT-4, Claude, or local LLM via Ollama |
| ğŸ¨ Visual Generator | Stable Diffusion, DALLÂ·E, Pika Labs, or Runway API |
| ğŸ”Š Voice Generator | ElevenLabs, Bark, Coqui, or Google TTS |
| ğŸ¬ Video Composer | FFmpeg, moviepy, or custom compositor |
| ğŸ–¥ï¸ Interface | CLI + optional GUI (PyQt or Tauri) |
| ğŸ§° Backend | Python 3.11+ (main), Node.js (if using GUI), Shell |
| ğŸ—ƒï¸ File Format | Outputs MP4 + JSON metadata per project |

---

## ğŸ”§ Setup

```bash
git clone https://github.com/makalin/Talk2Video.git
cd Talk2Video
pip install -r requirements.txt
# Optional: setup API keys in .env
```

---

## ğŸ—£ï¸ Example Usage

### CLI

```bash
python talk2video.py --input "A calm beach at sunset, narrated in a soft female voice"
```

### Voice Mode

```bash
python talk2video.py --mic
# Speak: "Create a cyberpunk city intro with deep voice narration"
```

### Multi-language Example

```bash
python examples/multilingual_scene.py
# Generates the same scene in multiple languages
```

### Turkish Example

```bash
python examples/turkish_scene.py
# Generates a scene with Turkish narration
```

---

## ğŸ“¦ API Keys Required

* `OPENAI_API_KEY` â€“ for GPT logic
* `ELEVENLABS_API_KEY` (or `GOOGLE_TTS_API_KEY`) â€“ for voice
* `RUNWAY_API_KEY` or `STABLE_DIFFUSION_HOST` â€“ for visuals
* `GOOGLE_APPLICATION_CREDENTIALS` â€“ for Google TTS (optional)

---

## ğŸ”Œ Modular Design

Want to use your own models or APIs? Just replace:

* `voice.py`
* `visuals.py`
* `logic.py`

---

## ğŸŒ Supported Languages

* ğŸ‡ºğŸ‡¸ English
* ğŸ‡ªğŸ‡¸ Spanish
* ğŸ‡«ğŸ‡· French
* ğŸ‡©ğŸ‡ª German
* ğŸ‡¯ğŸ‡µ Japanese
* ğŸ‡¹ğŸ‡· Turkish

Each language supports multiple voice providers and customizable parameters.

---

## ğŸ¨ Scene Styles

* Cinematic
* Documentary
* Commercial
* Educational
* Dramatic

---

## ğŸ§ª Roadmap

* [ ] Scene transitions & camera effects
* [ ] Avatar lip-sync support
* [ ] Additional language support
* [ ] Built-in storyboarding
* [ ] Mobile app (React Native or Flutter)
* [ ] Real-time voice input processing
* [ ] Custom voice model training

---

## ğŸ¤– License

MIT License â€” free to use, modify, and contribute.

---

## ğŸ™Œ Contribute

Pull requests welcome!
Let's make storytelling with voice easy for everyone.

---

## ğŸ“š Documentation

* [English](README.md)
* [Turkish](README.tr.md)
